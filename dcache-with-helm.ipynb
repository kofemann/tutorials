{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9a196b",
   "metadata": {},
   "source": [
    "# Simple tutorial on how to run dCache in k8s\n",
    "\n",
    "This tutorial will guide you through the process of deploying dCache in a k8s cluster. \n",
    "We will use the official dCache docker images and helm charts to deploy the dCache services.\n",
    "\n",
    "\n",
    "> Notes to notebook: This notebook requires bash kernel that can be installed, if needed:\n",
    "```bash\n",
    "pip install bash_kernel\n",
    "python -m bash_kernel.install\n",
    "```\n",
    "\n",
    "The 3rd-party dependencies as zookeeper, postgresql and kafka started in k8s too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcad526",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- A running k8s cluster\n",
    "- Helm installed\n",
    "- kubectl installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa328",
   "metadata": {},
   "source": [
    "## Create a namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f652cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/dcache-test created\n"
     ]
    }
   ],
   "source": [
    "PATH=$PATH:~/bin\n",
    "export K8S_NAMESPACE=dcache-test\n",
    "export KUBECONFIG=~/.kube/config\n",
    "kubectl create namespace ${K8S_NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59bf49be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          STATUS   AGE\n",
      "cattle-fleet-system           Active   46d\n",
      "cattle-impersonation-system   Active   46d\n",
      "cattle-system                 Active   46d\n",
      "ceph-csi-rbd                  Active   43d\n",
      "dcache-build-117653           Active   24h\n",
      "dcache-dev-infrastructure     Active   43d\n",
      "dcache-gitlab-runner          Active   43d\n",
      "default                       Active   46d\n",
      "kube-node-lease               Active   46d\n",
      "kube-public                   Active   46d\n",
      "kube-system                   Active   46d\n",
      "local                         Active   46d\n",
      "tigran-test                   Active   24h\n"
     ]
    }
   ],
   "source": [
    "kubectl get ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585185cf",
   "metadata": {},
   "source": [
    "## Add Helm repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bfdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bitnami\" has been added to your repositories\n",
      "\"dcache\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"dcache\" chart repository\n",
      "...Successfully got an update from the \"bitnami\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo add dcache https://gitlab.desy.de/api/v4/projects/7648/packages/helm/test\n",
    "helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d6d05",
   "metadata": {},
   "source": [
    "## Deploy external services\n",
    "\n",
    "Pinpoint PostgreSQL and Kafka to specific versions to avoid compatibility issues.\n",
    "\n",
    "> Despite the fact, that dCache wotks with other versions of PostgreSQL and Kafka, the _helm_ charts are incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcefab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: chimera\n",
      "LAST DEPLOYED: Tue Jun  4 21:52:15 2024\n",
      "NAMESPACE: dcache-test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "CHART NAME: postgresql\n",
      "CHART VERSION: 12.12.10\n",
      "APP VERSION: 15.4.0\n",
      "\n",
      "** Please be patient while the chart is being deployed **\n",
      "\n",
      "PostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:\n",
      "\n",
      "    chimera-postgresql.dcache-test.svc.cluster.local - Read/Write connection\n",
      "\n",
      "To get the password for \"postgres\" run:\n",
      "\n",
      "    export POSTGRES_ADMIN_PASSWORD=$(kubectl get secret --namespace dcache-test chimera-postgresql -o jsonpath=\"{.data.postgres-password}\" | base64 -d)\n",
      "\n",
      "To get the password for \"dcache\" run:\n",
      "\n",
      "    export POSTGRES_PASSWORD=$(kubectl get secret --namespace dcache-test chimera-postgresql -o jsonpath=\"{.data.password}\" | base64 -d)\n",
      "\n",
      "To connect to your database run the following command:\n",
      "\n",
      "    kubectl run chimera-postgresql-client --rm --tty -i --restart='Never' --namespace dcache-test --image docker.io/bitnami/postgresql:15.4.0-debian-11-r45 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" \\\n",
      "      --command -- psql --host chimera-postgresql -U dcache -d chimera -p 5432\n",
      "\n",
      "    > NOTE: If you access the container using bash, make sure that you execute \"/opt/bitnami/scripts/postgresql/entrypoint.sh /bin/bash\" in order to avoid the error \"psql: local user with ID 1001} does not exist\"\n",
      "\n",
      "To connect to your database from outside the cluster execute the following commands:\n",
      "\n",
      "    kubectl port-forward --namespace dcache-test svc/chimera-postgresql 5432:5432 &\n",
      "    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U dcache -d chimera -p 5432\n",
      "\n",
      "WARNING: The configured password will be ignored on new installation in case when previous PostgreSQL release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue.\n",
      "NAME: cells\n",
      "LAST DEPLOYED: Tue Jun  4 21:52:37 2024\n",
      "NAMESPACE: dcache-test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "CHART NAME: zookeeper\n",
      "CHART VERSION: 13.4.1\n",
      "APP VERSION: 3.9.2\n",
      "\n",
      "** Please be patient while the chart is being deployed **\n",
      "\n",
      "ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:\n",
      "\n",
      "    cells-zookeeper.dcache-test.svc.cluster.local\n",
      "\n",
      "To connect to your ZooKeeper server run the following commands:\n",
      "\n",
      "    export POD_NAME=$(kubectl get pods --namespace dcache-test -l \"app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=cells,app.kubernetes.io/component=zookeeper\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
      "    kubectl exec -it $POD_NAME -- zkCli.sh\n",
      "\n",
      "To connect to your ZooKeeper server from outside the cluster execute the following commands:\n",
      "\n",
      "    kubectl port-forward --namespace dcache-test svc/cells-zookeeper 2181:2181 &\n",
      "    zkCli.sh 127.0.0.1:2181\n",
      "\n",
      "WARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n",
      "  - resources\n",
      "  - tls.resources\n",
      "+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n",
      "NAME: billing\n",
      "LAST DEPLOYED: Tue Jun  4 21:53:09 2024\n",
      "NAMESPACE: dcache-test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "CHART NAME: kafka\n",
      "CHART VERSION: 23.0.7\n",
      "APP VERSION: 3.5.1\n",
      "\n",
      "** Please be patient while the chart is being deployed **\n",
      "\n",
      "Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:\n",
      "\n",
      "    billing-kafka.dcache-test.svc.cluster.local\n",
      "\n",
      "Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:\n",
      "\n",
      "    billing-kafka-0.billing-kafka-headless.dcache-test.svc.cluster.local:9092\n",
      "\n",
      "To create a pod that you can use as a Kafka client run the following commands:\n",
      "\n",
      "    kubectl run billing-kafka-client --restart='Never' --image docker.io/bitnami/kafka:3.5.1-debian-11-r1 --namespace dcache-test --command -- sleep infinity\n",
      "    kubectl exec --tty -i billing-kafka-client --namespace dcache-test -- bash\n",
      "\n",
      "    PRODUCER:\n",
      "        kafka-console-producer.sh \\\n",
      "            --broker-list billing-kafka-0.billing-kafka-headless.dcache-test.svc.cluster.local:9092 \\\n",
      "            --topic test\n",
      "\n",
      "    CONSUMER:\n",
      "        kafka-console-consumer.sh \\\n",
      "            --bootstrap-server billing-kafka.dcache-test.svc.cluster.local:9092 \\\n",
      "            --topic test \\\n",
      "            --from-beginning\n"
     ]
    }
   ],
   "source": [
    "helm -n ${K8S_NAMESPACE} install --wait --set auth.username=dcache \\\n",
    "                --set auth.password=let-me-in --set auth.database=chimera  \\\n",
    "                chimera bitnami/postgresql --version=12.12.10\n",
    "\n",
    "helm -n ${K8S_NAMESPACE} install --wait cells bitnami/zookeeper\n",
    "helm -n ${K8S_NAMESPACE} install --wait --set externalZookeeper.servers=cells-zookeeper \\\n",
    "                 --set kraft.enabled=false billing bitnami/kafka --version 23.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1477a8d",
   "metadata": {},
   "source": [
    "## List the running helm charts and k8s pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede06b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   \tNAMESPACE  \tREVISION\tUPDATED                                 \tSTATUS  \tCHART              \tAPP VERSION\n",
      "billing\tdcache-test\t1       \t2024-06-04 21:53:09.224625688 +0200 CEST\tdeployed\tkafka-23.0.7       \t3.5.1      \n",
      "cells  \tdcache-test\t1       \t2024-06-04 21:52:37.223519598 +0200 CEST\tdeployed\tzookeeper-13.4.1   \t3.9.2      \n",
      "chimera\tdcache-test\t1       \t2024-06-04 21:52:15.507075883 +0200 CEST\tdeployed\tpostgresql-12.12.10\t15.4.0     \n"
     ]
    }
   ],
   "source": [
    "helm -n ${K8S_NAMESPACE} list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9460e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS    RESTARTS   AGE\n",
      "billing-kafka-0        1/1     Running   0          116s\n",
      "cells-zookeeper-0      1/1     Running   0          2m28s\n",
      "chimera-postgresql-0   1/1     Running   0          2m50s\n"
     ]
    }
   ],
   "source": [
    "kubectl -n ${K8S_NAMESPACE} get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5ec48",
   "metadata": {},
   "source": [
    "## Deploy dCache\n",
    "\n",
    "Starting version 9.2.0 dcache containers are available on docker hub.\n",
    "\n",
    "This will start a pre-configured dCache cluster named `store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee4c446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: store\n",
      "LAST DEPLOYED: Tue Jun  4 21:55:41 2024\n",
      "NAMESPACE: dcache-test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "helm -n ${K8S_NAMESPACE} install --wait --set image.tag=10.0.3 store dcache/dcache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a27cd",
   "metadata": {},
   "source": [
    "### Get list of running services for door and pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d691fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               AGE\n",
      "store-door-svc   ClusterIP   10.43.78.54   <none>        2049/TCP,22125/TCP,22128/TCP,1094/TCP,1095/TCP,8080/TCP,8083/TCP,2811/TCP,8443/TCP,22224/TCP,3880/TCP,11111/TCP,28000/TCP,28001/TCP,28002/TCP,28003/TCP,28004/TCP,28005/TCP,28006/TCP,28007/TCP,28008/TCP,28009/TCP,28010/TCP,28011/TCP,28012/TCP,28013/TCP,28014/TCP,28015/TCP,28016/TCP,28017/TCP,28018/TCP,28019/TCP,28020/TCP,28021/TCP,28022/TCP,28023/TCP,28024/TCP,28025/TCP,28026/TCP,28027/TCP,28028/TCP,28029/TCP,28030/TCP,28031/TCP,28032/TCP,28033/TCP,28034/TCP,28035/TCP,28036/TCP,28037/TCP,28038/TCP,28039/TCP,28040/TCP,28041/TCP,28042/TCP,28043/TCP,28044/TCP,28045/TCP,28046/TCP,28047/TCP,28048/TCP,28049/TCP   8m59s\n"
     ]
    }
   ],
   "source": [
    "kubectl -n ${K8S_NAMESPACE} get services -l=app=door"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "299fc9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       AGE\n",
      "store-pool-a-svc   ClusterIP   10.43.14.206   <none>        32049/TCP,31094/TCP,38080/TCP,38083/TCP,28000/TCP,28001/TCP,28002/TCP,28003/TCP,28004/TCP,28005/TCP,28006/TCP,28007/TCP,28008/TCP,28009/TCP,28010/TCP,28011/TCP,28012/TCP,28013/TCP,28014/TCP,28015/TCP,28016/TCP,28017/TCP,28018/TCP,28019/TCP,28020/TCP,28021/TCP,28022/TCP,28023/TCP,28024/TCP,28025/TCP,28026/TCP,28027/TCP,28028/TCP,28029/TCP,28030/TCP,28031/TCP,28032/TCP,28033/TCP,28034/TCP,28035/TCP,28036/TCP,28037/TCP,28038/TCP,28039/TCP,28040/TCP,28041/TCP,28042/TCP,28043/TCP,28044/TCP,28045/TCP,28046/TCP,28047/TCP,28048/TCP,28049/TCP   9m32s\n"
     ]
    }
   ],
   "source": [
    "kubectl -n ${K8S_NAMESPACE} get services -l=app=pool-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0b08e",
   "metadata": {},
   "source": [
    "## Add a worker node\n",
    "\n",
    "Add a worker that runs in the same namespace as the dCache services and\n",
    "can be used to test the dCache services. The worker nod just sits there and does nothing :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a7c213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/wn created\n"
     ]
    }
   ],
   "source": [
    "kubectl -n ${K8S_NAMESPACE} run wn --image almalinux:9 -- /bin/sleep inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb1dc9",
   "metadata": {},
   "source": [
    "### Access the worker node with exec and run some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0f6554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mDate\u001b[0m: Tue, 04 Jun 2024 20:20:49 GMT\n",
      "\u001b[1mServer\u001b[0m: dCache/10.0.3\n",
      "\u001b[1mAccept-Ranges\u001b[0m: bytes\n",
      "\u001b[1mETag\u001b[0m: \"0000569948F92CB14E18AFE7760C48F69956_-455864921\"\n",
      "\u001b[1mLast-Modified\u001b[0m: Tue, 04 Jun 2024 19:57:33 GMT\n",
      "\u001b[1mLink\u001b[0m: <https://store-door-svc:8083/data?type=metalink>; rel=describedby; type=\"application/metalink4+xml\"\n",
      "\u001b[1mContent-Type\u001b[0m: text/html;charset=utf-8\n",
      "\u001b[1mTransfer-Encoding\u001b[0m: chunked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kubectl -n ${K8S_NAMESPACE} exec -ti wn -- curl -k --head https://store-door-svc:8083/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bd9e2",
   "metadata": {},
   "source": [
    "## Some other staff that works in  terminal, but not in a notebook\n",
    "\n",
    "\n",
    "### Access dCache admin interface\n",
    "\n",
    "```bash\n",
    "kubectl -n ${K8S_NAMESPACE} run --rm -ti admin-ssh --image kroniak/ssh-client -- /bin/bash\n",
    "```\n",
    "\n",
    "\n",
    "### An interactive WN session.\n",
    "```bash\n",
    "kubectl -n ${K8S_NAMESPACE} exec -ti wn -- /bin/bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1591ba",
   "metadata": {},
   "source": [
    "## Delete all and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f8e4118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace \"dcache-test\" deleted\n"
     ]
    }
   ],
   "source": [
    "kubectl delete namespace ${K8S_NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92495c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
